file_name: gpt2-small-124M.pth
vocab_size: 50257
context_len: 1024
drop_rate: 0.0
bias: True
emb_dim: 768
n_heads: 12
n_layers: 12
